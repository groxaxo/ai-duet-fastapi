# OpenAI API Configuration (used for LLM)
# Can also work with OpenAI-compatible endpoints (Fireworks, Ollama, etc.)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=
# Examples for OPENAI_BASE_URL:
# For Fireworks: https://api.fireworks.ai/inference/v1
# For Ollama: http://localhost:11434/v1

# LLM Configuration
LLM_API=chat                    # "responses" or "chat"
LLM_MODEL=gpt-4o-mini          # Model name (gpt-4o-mini, llama3.2, etc.)
LLM_TEMPERATURE=0.7            # Temperature for LLM generation

# STT/TTS Provider Configuration
STT_PROVIDER=local             # "local" (faster-whisper) or "deepinfra"
TTS_PROVIDER=local             # "local" (kokoro) or "deepinfra"

# DeepInfra Configuration (if using deepinfra providers)
DEEPINFRA_API_KEY=your_deepinfra_api_key_here
DEEPINFRA_WHISPER_MODEL=openai/whisper-large-v3-turbo
# Available TTS models: hexgrad/Kokoro-82M, resemblyai/chatterbox-turbo, canopylabs/orpheus-3b-0.1-ft
DEEPINFRA_TTS_MODEL=hexgrad/Kokoro-82M
KOKORO_LANG=a                  # Language code for Kokoro (only used with Kokoro model)

# Local Whisper Configuration (if using local STT)
WHISPER_MODEL=small            # tiny, base, small, medium, large
WHISPER_DEVICE=cpu             # cpu or cuda
WHISPER_COMPUTE=int8           # float16, int8, int8_float16

# Memory Configuration (Memvid)
MEMORY_DEFAULT_ENABLED=true    # Enable memory by default
MEMORY_DEFAULT_K=6             # Number of memory results to retrieve
MEMORY_DEFAULT_MODE=lex        # lex (lexical) or sem (semantic)