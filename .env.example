# LLM Provider Configuration
# Choose provider: "openai", "fireworks", or "ollama"
LLM_PROVIDER=fireworks

# OpenAI Configuration (if using OpenAI or compatible endpoints)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=  # Optional: custom endpoint URL (e.g., for Ollama: http://localhost:11434/v1)

# Fireworks AI API Key (for LLM)
# Get from: https://fireworks.ai
FIREWORKS_API_KEY=your_fireworks_api_key_here

# LLM Model Configuration
LLM_MODEL=accounts/fireworks/models/llama-v3p1-405b-instruct
LLM_TEMPERATURE=0.7

# DeepInfra API Key (for TTS)
# Get from: https://deepinfra.com
DEEPINFRA_API_KEY=your_deepinfra_api_key_here

# Whisper Model Configuration (optional)
WHISPER_MODEL_SIZE=small      # tiny, base, small, medium, large
WHISPER_DEVICE=cpu           # cpu or cuda
WHISPER_COMPUTE_TYPE=int8    # float16, int8, int8_float16

# Server Configuration (optional)
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info